{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2c4720",
   "metadata": {},
   "source": [
    "# 2.3.2 Updatable Featurization Jobs and Datasets (tg.common.datasets.featurization.updatable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638c380",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "If the data in the dataset are changing, we may want to update dataset from time to time. However, we don't want to reprocess all the records, as it takes too much time. This problem is addressed by the `UpdatableDataset` class and jobs that create and process it.\n",
    "\n",
    "The dataset consists of _revisions_. Each revision is essentially a dataframe, that may be partitioned into smaller dataframes for optimization reasons (as done in `FeaturizationJob`). Rows of each revision describes the entities (like passengers, customers, or articles), and the dataframe index must be a unique identifier for the entity. If several rows are available in different revisions for the same entity, the later takes precedence. \n",
    "\n",
    "When stored in S3 or other external storages, several datasets may be stored together the same way as we have seen in the `FeaturizationJob`: the data are the same, but processed by different featurizers. \n",
    "\n",
    "Revisions may be _major_ and _minor_. Major revision corresponds to reevauation of all entities. It **does not** physically remove data from the dataset, but effectively voids them - data older that latest major revision are not read by default. The minor revisions are supposed to be small updates that rolls on top of the major revision. Due to this design:\n",
    "\n",
    "* it is possible to restore the dataset's state to any given time in the past. \n",
    "* it is also possible not to carry around the very old data all the time, as they can be voided by a major revision.\n",
    "\n",
    "The structure of the files on the S3 or other storages is:\n",
    "\n",
    "```\n",
    "root\n",
    "↳ revision_1\n",
    "  ↳ featurizer_X\n",
    "    ↳ partition_1_X_i.parquet\n",
    "    ↳ partition_1_X_ii.parquet\n",
    "  ↳ featurizer_Y\n",
    "    ↳ partition_1_Y_i.parquet\n",
    "    ↳ partition_1_Y_ii.parquet\n",
    "↳ revision_2\n",
    "  ↳ featurizer_X\n",
    "    ↳ partition_2_X_i.parquet\n",
    "    ↳ partition_2_X_ii.parquet\n",
    "  ↳ featurizer_Y\n",
    "    ↳ partition_2_Y_i.parquet\n",
    "    ↳ partition_2_Y_ii.parquet\n",
    "↳ description.parquet\n",
    "```\n",
    "\n",
    "`revision_*` folders and `partition_*` files are  usually GUIDs, `featurizer_*` have meaningful names. \n",
    "\n",
    "`description.parquet` is the file describing the revisions. For each revision we know:\n",
    "\n",
    "* the timestamp it was produced at\n",
    "* if the revision was major or minor\n",
    "* and the version of the job that has produced the revision. If the version was updated, the major revision should be triggered.\n",
    "\n",
    "Also, `description.parquet` plays a role of transactions keeper: all the jobs producing datasets upload this file at the very end. Since this file keeps record of others, incomplete upload will not have any effect (aside from consuming space on the remote storage).\n",
    "\n",
    "When working with this dataset, we specify the time `T` and download revisions between `T` and the last major revision before `T`. We also specify one featurizer, and download data for that. The resulting local file structure is:\n",
    "\n",
    "```\n",
    "root\n",
    "↳ revision_1\n",
    "  ↳ partition_1_X_i.parquet\n",
    "  ↳ partition_1_X_ii.parquet\n",
    "↳ revision_2\n",
    "  ↳ partition_2_X_i.parquet\n",
    "  ↳ partition_2_X_ii.parquet\n",
    "↳ description.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5312b180",
   "metadata": {},
   "source": [
    "## Creating and reading updatable datasets\n",
    "\n",
    "To demonstrate the file structure above, we will create an artificial dataset witout any particular meaning. The simplest way to do it is using a method in `UpdatableDataset` class. As before, we will use `MemoryFileSyncer` to demonstrate the file structure on the remote host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "427aa76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:19.277840Z",
     "iopub.status.busy": "2022-12-28T13:20:19.277158Z",
     "iopub.status.idle": "2022-12-28T13:20:20.024255Z",
     "shell.execute_reply": "2022-12-28T13:20:20.024769Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.common.datasets.featurization import UpdatableDataset\n",
    "from tg.common import MemoryFileSyncer\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "mem = MemoryFileSyncer()\n",
    "\n",
    "def time(day):\n",
    "    return datetime(2020,1,1+day)\n",
    "\n",
    "def create_dataframe(day, featurizer):\n",
    "    N=5\n",
    "    return pd.DataFrame(dict(\n",
    "        day = [day] * N,\n",
    "        featurizer = [featurizer] * N,\n",
    "        id = list(range(day, day+N))\n",
    "    )).set_index(\"id\")\n",
    "\n",
    "for day in [0,2,4,6]:\n",
    "    record = UpdatableDataset.DescriptionItem(\n",
    "        name = f'revision_{day}',\n",
    "        timestamp = time(day),\n",
    "        is_major = day%4==0,\n",
    "        version = '')\n",
    "    data = {featurizer: create_dataframe(day, featurizer) \n",
    "            for featurizer in ['featurizer_A','featurizer_B']}\n",
    "    UpdatableDataset.write_to_updatable_dataset(\n",
    "        syncer = mem,\n",
    "        record = record,\n",
    "        data = data\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e8bfc",
   "metadata": {},
   "source": [
    "Here is the list of files and folders, created in the `MemoryFileSyncer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "044023f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.030734Z",
     "iopub.status.busy": "2022-12-28T13:20:20.030137Z",
     "iopub.status.idle": "2022-12-28T13:20:20.032427Z",
     "shell.execute_reply": "2022-12-28T13:20:20.032939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['revision_0/featurizer_B/data.parquet',\n",
       " 'revision_0/featurizer_A/data.parquet',\n",
       " 'description.parquet',\n",
       " 'revision_2/featurizer_B/data.parquet',\n",
       " 'revision_2/featurizer_A/data.parquet',\n",
       " 'revision_4/featurizer_B/data.parquet',\n",
       " 'revision_4/featurizer_A/data.parquet',\n",
       " 'revision_6/featurizer_B/data.parquet',\n",
       " 'revision_6/featurizer_A/data.parquet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mem.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ca2baa",
   "metadata": {},
   "source": [
    "The description of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7223940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.037807Z",
     "iopub.status.busy": "2022-12-28T13:20:20.037116Z",
     "iopub.status.idle": "2022-12-28T13:20:20.046272Z",
     "shell.execute_reply": "2022-12-28T13:20:20.046588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_major</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>revision_0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>revision_2</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>revision_4</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>revision_6</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  timestamp  is_major version\n",
       "0  revision_0 2020-01-01      True        \n",
       "1  revision_2 2020-01-03     False        \n",
       "2  revision_4 2020-01-05      True        \n",
       "3  revision_6 2020-01-07     False        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(mem.get_file_stream('description.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0dc5e",
   "metadata": {},
   "source": [
    "Here is how one revision looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc915b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.050301Z",
     "iopub.status.busy": "2022-12-28T13:20:20.049624Z",
     "iopub.status.idle": "2022-12-28T13:20:20.056640Z",
     "shell.execute_reply": "2022-12-28T13:20:20.056273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day    featurizer\n",
       "id                   \n",
       "0     0  featurizer_A\n",
       "1     0  featurizer_A\n",
       "2     0  featurizer_A\n",
       "3     0  featurizer_A\n",
       "4     0  featurizer_A"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(mem.get_file_stream('revision_0/featurizer_A/data.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a07c503",
   "metadata": {},
   "source": [
    "Here is the table, describing our dataset for `featurizer_A`: in columns, there are revisions, in rows, entities, and in cells, the `day` field of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea32792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.062722Z",
     "iopub.status.busy": "2022-12-28T13:20:20.061959Z",
     "iopub.status.idle": "2022-12-28T13:20:20.092846Z",
     "shell.execute_reply": "2022-12-28T13:20:20.092502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_0</th>\n",
       "      <th>day_2</th>\n",
       "      <th>day_4</th>\n",
       "      <th>day_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day_0  day_2  day_4  day_6\n",
       "id                            \n",
       "0     0.0    NaN    NaN    NaN\n",
       "1     0.0    NaN    NaN    NaN\n",
       "2     0.0    2.0    NaN    NaN\n",
       "3     0.0    2.0    NaN    NaN\n",
       "4     0.0    2.0    4.0    NaN\n",
       "5     NaN    2.0    4.0    NaN\n",
       "6     NaN    2.0    4.0    6.0\n",
       "7     NaN    NaN    4.0    6.0\n",
       "8     NaN    NaN    4.0    6.0\n",
       "9     NaN    NaN    NaN    6.0\n",
       "10    NaN    NaN    NaN    6.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for day in [0,2,4,6]:\n",
    "    df = pd.read_parquet(mem.get_file_stream(f\"revision_{day}/featurizer_A/data.parquet\"))\n",
    "    dfs.append(df[['day']].rename(columns={'day':f'day_{day}'}))\n",
    "summary_df = pd.concat(dfs,axis=1)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb80a8c",
   "metadata": {},
   "source": [
    "Use this table for reference: we will now read this dataset at specific time periods, so you will be able to see which records are available, and at which time.\n",
    "\n",
    "Now, let's read the dataset with the `UpdatableDataset` class. It will \"download\" the data to the local drive (in this particular example, it downloads from memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22608995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.096351Z",
     "iopub.status.busy": "2022-12-28T13:20:20.095705Z",
     "iopub.status.idle": "2022-12-28T13:20:20.119447Z",
     "shell.execute_reply": "2022-12-28T13:20:20.119785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day    featurizer\n",
       "id                   \n",
       "6     6  featurizer_A\n",
       "7     6  featurizer_A\n",
       "8     6  featurizer_A\n",
       "9     6  featurizer_A\n",
       "10    6  featurizer_A\n",
       "4     4  featurizer_A\n",
       "5     4  featurizer_A"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = UpdatableDataset(\n",
    "    location = './temp/updatable_dataset',\n",
    "    featurizer_name = 'featurizer_A',\n",
    "    syncer = mem\n",
    ")\n",
    "\n",
    "dataset.read(cache_mode='remake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a30963",
   "metadata": {},
   "source": [
    "**Note** the `download` argument. In the normal setup, we download dataset to the appropriate timeframe once with `download` method, and then just read from it. In this paragraph however we want to demonstrate downloading from different timeframes, so we will have to download with each read.\n",
    "\n",
    "This is the \"current\" state of the dataset: \n",
    "* records 9-10 were added in the last minor revision (6)\n",
    "* records 6-8 were updated in the last minor revision (6)\n",
    "* records 4-5 were inherited from the revision (4)\n",
    "* records 1-3 are lost, because the revision (4) is major"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c374c6",
   "metadata": {},
   "source": [
    "This is the list of local files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47d9bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.123740Z",
     "iopub.status.busy": "2022-12-28T13:20:20.122780Z",
     "iopub.status.idle": "2022-12-28T13:20:20.128132Z",
     "shell.execute_reply": "2022-12-28T13:20:20.128496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp/updatable_dataset/revision_4\n",
      "temp/updatable_dataset/description.parquet\n",
      "temp/updatable_dataset/revision_6\n",
      "temp/updatable_dataset/revision_4/featurizer_A\n",
      "temp/updatable_dataset/revision_4/featurizer_A/data.parquet\n",
      "temp/updatable_dataset/revision_6/featurizer_A\n",
      "temp/updatable_dataset/revision_6/featurizer_A/data.parquet\n"
     ]
    }
   ],
   "source": [
    "from yo_fluq_ds import *\n",
    "\n",
    "Query.folder('./temp/updatable_dataset','**/*').foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f9764",
   "metadata": {},
   "source": [
    "As we can see, indeed `UpdatableDataset` has only downloaded what is needed for the provided timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a9afc1",
   "metadata": {},
   "source": [
    "`UpdatableDataset` can restore it's state to any given point in the past:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2054582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.133060Z",
     "iopub.status.busy": "2022-12-28T13:20:20.132298Z",
     "iopub.status.idle": "2022-12-28T13:20:20.162681Z",
     "shell.execute_reply": "2022-12-28T13:20:20.163945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day    featurizer\n",
       "id                   \n",
       "0     0  featurizer_A\n",
       "1     0  featurizer_A\n",
       "2     0  featurizer_A\n",
       "3     0  featurizer_A\n",
       "4     0  featurizer_A"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.read(to_timestamp=time(1), cache_mode='remake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f38976",
   "metadata": {},
   "source": [
    "As we see, \"lost\" records 1-3 are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc95fd61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.170252Z",
     "iopub.status.busy": "2022-12-28T13:20:20.169498Z",
     "iopub.status.idle": "2022-12-28T13:20:20.190745Z",
     "shell.execute_reply": "2022-12-28T13:20:20.191252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day    featurizer\n",
       "id                   \n",
       "2     2  featurizer_A\n",
       "3     2  featurizer_A\n",
       "4     2  featurizer_A\n",
       "5     2  featurizer_A\n",
       "6     2  featurizer_A\n",
       "0     0  featurizer_A\n",
       "1     0  featurizer_A"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.read(to_timestamp=time(3), cache_mode='remake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394888e",
   "metadata": {},
   "source": [
    "Also, `UpdatableDataset` can provide you with changes, made between a given timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c27b47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.195709Z",
     "iopub.status.busy": "2022-12-28T13:20:20.195010Z",
     "iopub.status.idle": "2022-12-28T13:20:20.210180Z",
     "shell.execute_reply": "2022-12-28T13:20:20.209663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day    featurizer\n",
       "id                   \n",
       "2     2  featurizer_A\n",
       "3     2  featurizer_A\n",
       "4     2  featurizer_A\n",
       "5     2  featurizer_A\n",
       "6     2  featurizer_A"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.read(from_timestamp=time(1), to_timestamp=time(3), cache_mode='remake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c42ce18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.213879Z",
     "iopub.status.busy": "2022-12-28T13:20:20.213203Z",
     "iopub.status.idle": "2022-12-28T13:20:20.228942Z",
     "shell.execute_reply": "2022-12-28T13:20:20.229451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>featurizer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>featurizer_A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day    featurizer\n",
       "id                   \n",
       "4     4  featurizer_A\n",
       "5     4  featurizer_A\n",
       "6     4  featurizer_A\n",
       "7     4  featurizer_A\n",
       "8     4  featurizer_A"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.read(from_timestamp=time(1), to_timestamp=time(5), cache_mode='remake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1ad88",
   "metadata": {},
   "source": [
    "Between the days 1 and 5, two revisions took place: (2) and (4). But, since the revision (4) is major, it voids the output of the revision (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77fc092",
   "metadata": {},
   "source": [
    "We encourage you to play with this code, downloading and reading data for different timeframes, and comparing with `summary_df` table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbc207",
   "metadata": {},
   "source": [
    "`UpdatableDataset` supports `count`, `selector` and `columns` arguments for reading, as well as `as_data_frame_source` method that converts this dataset to the `DataFrameSource` interface. The guidelines of dataset exploration are the same as for `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab0254",
   "metadata": {},
   "source": [
    "## Updatable Featurization Job\n",
    "\n",
    "`UpdableFeaturizationJob` is an analogue to `FeaturizationJob` that supports updates. The arguments of this class are essentially the same as for `FeaturizationJob`, because under the hood `UpdatableFeaturizationJob` only sorts out the timeframes, and spawns `FeaturizationJob` instances for actual featurization.\n",
    "\n",
    "The only difference lies with `DataSource` that the job consumes. `UpdatableFeaturizationJob` requires you to provide a full data source that is used in major updates. If you want to have minor updates, you must also provide a factory that created a `DataSource` for a given timeframe.\n",
    "\n",
    "Let's create an `UpdatableFeaturizationJob` for the Titanic dataset. Since this dataset doesn't actually have the time dependency, we will just split it into 3 groups, by `Embarked` field:\n",
    "* for the initial major revision, `Embarked` will be set to None for all passengers (as if made before the trip started)\n",
    "* following revisions will add the `Embarked` fields\n",
    "\n",
    "The `Embarked` field is distributed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8324afd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.233666Z",
     "iopub.status.busy": "2022-12-28T13:20:20.233048Z",
     "iopub.status.idle": "2022-12-28T13:20:20.247041Z",
     "shell.execute_reply": "2022-12-28T13:20:20.247621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embarked\n",
       "C       168\n",
       "NONE      2\n",
       "Q        77\n",
       "S       644\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df.groupby(df.Embarked.fillna(\"NONE\")).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7507e",
   "metadata": {},
   "source": [
    "The data sources are implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c52ee31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.256027Z",
     "iopub.status.busy": "2022-12-28T13:20:20.254688Z",
     "iopub.status.idle": "2022-12-28T13:20:20.258990Z",
     "shell.execute_reply": "2022-12-28T13:20:20.259726Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.common.datasets.access import DataSource\n",
    "\n",
    "class EmbarkedDataSource(DataSource):\n",
    "    def __init__(self, embarked):\n",
    "        self.embarked = embarked\n",
    "        \n",
    "    def get_data(self):\n",
    "        df = pd.read_csv('titanic.csv')\n",
    "        if self.embarked is None:\n",
    "            df.Embarked = 'NONE'\n",
    "        else:\n",
    "            df = df.loc[df.Embarked == self.embarked]\n",
    "        return Query.df(df)\n",
    "        \n",
    "def source_factory(from_timestamp, to_timestamp):\n",
    "    if to_timestamp.day == 3:\n",
    "        return EmbarkedDataSource('C')\n",
    "    elif to_timestamp.day == 5:\n",
    "        return EmbarkedDataSource('Q')\n",
    "    else:\n",
    "        return EmbarkedDataSource('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1f8bb",
   "metadata": {},
   "source": [
    "Now we will implement and run `UpdatableFeaturizationJob`. Note that we must take some action regarding the dataframe indices: before we were simply ignoring it, but now we have to make sure that index is a unique identifier for an entity, i.e., passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "463a0e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.266850Z",
     "iopub.status.busy": "2022-12-28T13:20:20.261898Z",
     "iopub.status.idle": "2022-12-28T13:20:20.491725Z",
     "shell.execute_reply": "2022-12-28T13:20:20.492045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.265386 INFO: Starting lesvik job test_featurization_job, version v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.266654 INFO: Additional settings limit NONE, reporting NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.267648 INFO: 0 previous revisions are found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.268154 INFO: Running with id 0 at 2020-01-01 00:00:00, revision is MAJOR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.268623 INFO: Featurization Job test_featurization_job at version v1 has started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.274809 INFO: Fetching data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.331211 INFO: Data fetched, finalizing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.342725 INFO: Uploading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.343953 INFO: Featurization job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.344573 INFO: 891 were processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.344993 INFO: Uploading new description\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.348811 INFO: Job finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.349338 INFO: Starting lesvik job test_featurization_job, version v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.349793 INFO: Additional settings limit NONE, reporting NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.353792 INFO: 1 previous revisions are found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.354260 INFO: Running with id 2 at 2020-01-03 00:00:00, revision is MINOR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.354596 INFO: Featurization Job test_featurization_job at version v1 has started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.359414 INFO: Fetching data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.369041 INFO: Data fetched, finalizing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.374701 INFO: Uploading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.375694 INFO: Featurization job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.376128 INFO: 168 were processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.376520 INFO: Uploading new description\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.379880 INFO: Job finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.380402 INFO: Starting lesvik job test_featurization_job, version v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.380797 INFO: Additional settings limit NONE, reporting NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.384814 INFO: 2 previous revisions are found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.385369 INFO: Running with id 4 at 2020-01-05 00:00:00, revision is MINOR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.385709 INFO: Featurization Job test_featurization_job at version v1 has started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.390244 INFO: Fetching data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.395039 INFO: Data fetched, finalizing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.401771 INFO: Uploading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.402909 INFO: Featurization job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.404464 INFO: 77 were processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.406484 INFO: Uploading new description\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.410661 INFO: Job finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.411241 INFO: Starting lesvik job test_featurization_job, version v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.411682 INFO: Additional settings limit NONE, reporting NONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.421033 INFO: 3 previous revisions are found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.422118 INFO: Running with id 6 at 2020-01-07 00:00:00, revision is MINOR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.422622 INFO: Featurization Job test_featurization_job at version v1 has started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.427384 INFO: Fetching data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.474042 INFO: Data fetched, finalizing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.482652 INFO: Uploading data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.483651 INFO: Featurization job completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.484254 INFO: 644 were processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.484739 INFO: Uploading new description\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-28 14:20:20.488453 INFO: Job finished\n"
     ]
    }
   ],
   "source": [
    "from tg.common.datasets.featurization import UpdatableFeaturizationJob, DataframeFeaturizer\n",
    "\n",
    "dataset_buffer = MemoryFileSyncer()\n",
    "\n",
    "class PassengerFeaturizer(DataframeFeaturizer):\n",
    "    def __init__(self):\n",
    "        super(PassengerFeaturizer, self).__init__(row_selector=lambda z: z)\n",
    "    \n",
    "    def _postprocess(self, df):\n",
    "        return df.set_index('PassengerId')\n",
    "    \n",
    "job = UpdatableFeaturizationJob(\n",
    "    name = 'test_featurization_job',\n",
    "    version = 'v1',\n",
    "    full_data_source=EmbarkedDataSource(None),\n",
    "    update_data_source_factory=source_factory,\n",
    "    featurizers = dict(passengers = PassengerFeaturizer()),\n",
    "    syncer = dataset_buffer,\n",
    "    limit = None,\n",
    "    reporting_frequency=None\n",
    ")\n",
    "\n",
    "for i in [0,2,4,6]:\n",
    "    job.run(current_time = time(i),custom_revision_id=str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf2132",
   "metadata": {},
   "source": [
    "I have given a meaningful names to revisions with `custom_uid` argument. This is not necessary in general, as the order of revision is reflected in `description.parquet`.\n",
    "\n",
    "Let's look at the revisions created. I will also add the information about records that were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbdbeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.498369Z",
     "iopub.status.busy": "2022-12-28T13:20:20.497697Z",
     "iopub.status.idle": "2022-12-28T13:20:20.531341Z",
     "shell.execute_reply": "2022-12-28T13:20:20.530731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_major</th>\n",
       "      <th>version</th>\n",
       "      <th>file</th>\n",
       "      <th>NONE</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>True</td>\n",
       "      <td>v1</td>\n",
       "      <td>9cee2b45-6757-4d05-a965-23bbe7156beb.parquet</td>\n",
       "      <td>891.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>False</td>\n",
       "      <td>v1</td>\n",
       "      <td>8877c73f-43db-49d6-8436-0088ffdc6a8b.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>False</td>\n",
       "      <td>v1</td>\n",
       "      <td>2d6d2392-9761-4822-b67e-53e00b81d60c.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>False</td>\n",
       "      <td>v1</td>\n",
       "      <td>032f775d-7f9f-4b40-9bb7-2cd72519d590.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  timestamp  is_major version  \\\n",
       "0    0 2020-01-01      True      v1   \n",
       "1    2 2020-01-03     False      v1   \n",
       "2    4 2020-01-05     False      v1   \n",
       "3    6 2020-01-07     False      v1   \n",
       "\n",
       "                                           file   NONE      C     Q      S  \n",
       "0  9cee2b45-6757-4d05-a965-23bbe7156beb.parquet  891.0    NaN   NaN    NaN  \n",
       "1  8877c73f-43db-49d6-8436-0088ffdc6a8b.parquet    NaN  168.0   NaN    NaN  \n",
       "2  2d6d2392-9761-4822-b67e-53e00b81d60c.parquet    NaN    NaN  77.0    NaN  \n",
       "3  032f775d-7f9f-4b40-9bb7-2cd72519d590.parquet    NaN    NaN   NaN  644.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "def get_embarkation_by_revision(buffer):\n",
    "    desc_df = pd.read_parquet(buffer.get_file_stream('description.parquet'))\n",
    "    rows = []\n",
    "    for key in buffer.cache:\n",
    "        if key == 'description.parquet':\n",
    "            continue\n",
    "        df = buffer.get_parquet(key)\n",
    "        df = df.groupby('Embarked').size().to_frame().transpose().iloc[0]\n",
    "        row = OrderedDict()\n",
    "        row['partition'] = key.split('/')[0]\n",
    "        row['file'] = key.split('/')[2]\n",
    "        for s in Query.series(df):\n",
    "            row[s.key] = s.value\n",
    "        \n",
    "        rows.append(row)\n",
    "        \n",
    "    edf = pd.DataFrame(rows)\n",
    "    desc_df = desc_df.merge(edf.set_index('partition'), left_on='name',right_index=True)\n",
    "    return desc_df\n",
    "\n",
    "desc_df = get_embarkation_by_revision(dataset_buffer)\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5531d8",
   "metadata": {},
   "source": [
    "As we see, all worked as expected.\n",
    "\n",
    "The created dataset can be then accessed via `UpdatableDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8c3f43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.536889Z",
     "iopub.status.busy": "2022-12-28T13:20:20.536482Z",
     "iopub.status.idle": "2022-12-28T13:20:20.581201Z",
     "shell.execute_reply": "2022-12-28T13:20:20.581520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>records_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NONE</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked  timestamp  records_count\n",
       "0        C 2020-01-03            168\n",
       "1     NONE 2020-01-01              2\n",
       "2        Q 2020-01-05             77\n",
       "3        S 2020-01-07            644"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = UpdatableDataset(\n",
    "    location = './temp/updatable_dataset_2',\n",
    "    featurizer_name = 'passengers',\n",
    "    syncer = dataset_buffer\n",
    ")\n",
    "df = dataset.read(cache_mode='remake', partition_name_column='partition_name')\n",
    "df = df.merge(\n",
    "    dataset.get_desription_as_df().set_index('name'),\n",
    "    left_on='partition_name',\n",
    "    right_index=True)\n",
    "df.groupby(['Embarked','timestamp']).size().to_frame('records_count').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83658078",
   "metadata": {},
   "source": [
    "The handy argument `partition_name_column` adds to each rows the name of the revision that has produced this row. This name may be then used to merge with the description for further technical information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130e8bc",
   "metadata": {},
   "source": [
    "## `UpdatableDatasetScoringJob`\n",
    "\n",
    "There are probably lots of scenarios how the UpdatableDatasets may be processed. So far, we have suppored one scenario, `UpdatableDatasetScoringJob`. This job is mostly designed to compute scores for entities in datasets for different purposes. So the actions performed are:\n",
    "\n",
    "* Get the last time when the job was run\n",
    "* Download updates from dataset(s) from this last time\n",
    "* Compute scores for the update\n",
    "* Upload the updates for the dataset\n",
    "\n",
    "Such job consists of UpdatableDatasetScoringMethods, each method knows:\n",
    "* The dataset it pulls data from\n",
    "* The function that must be applied to the resulting dataframe. The function must preserve the dataframe index.\n",
    "\n",
    "Let's define such scoring method and job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "141c1e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.585999Z",
     "iopub.status.busy": "2022-12-28T13:20:20.583495Z",
     "iopub.status.idle": "2022-12-28T13:20:20.655732Z",
     "shell.execute_reply": "2022-12-28T13:20:20.656253Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.common.datasets.featurization import UpdatableDatasetScoringJob, UpdatableDatasetScoringMethod\n",
    "from datetime import timedelta\n",
    "\n",
    "def compute_scores(df):\n",
    "    return df[['Survived','Embarked']]\n",
    "\n",
    "scores_buffer = MemoryFileSyncer()\n",
    "\n",
    "job = UpdatableDatasetScoringJob(\n",
    "    name = 'scoring',\n",
    "    version = '',\n",
    "    dst_syncer = scores_buffer,\n",
    "    methods = [\n",
    "        UpdatableDatasetScoringMethod(\n",
    "            'passenger_scores',\n",
    "            dataset_buffer,\n",
    "            'passengers',\n",
    "            compute_scores        \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in [0,2,4,6]:\n",
    "    job.run(current_time = time(i)+timedelta(hours=1), custom_revision_id=str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4461ec31",
   "metadata": {},
   "source": [
    "The addition of one hour is required to reflect the fact that the scoring job would need to run _after_ the initial job. The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b38b172a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.660299Z",
     "iopub.status.busy": "2022-12-28T13:20:20.659649Z",
     "iopub.status.idle": "2022-12-28T13:20:20.695348Z",
     "shell.execute_reply": "2022-12-28T13:20:20.695702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_major</th>\n",
       "      <th>version</th>\n",
       "      <th>file</th>\n",
       "      <th>NONE</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>e94bf29e-9cc4-42ef-8f06-0315a822d532.parquet</td>\n",
       "      <td>891.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-03 01:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>2bd928b5-f354-4958-9410-746774ea2d0e.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-05 01:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>20ebdf31-5b8b-4f4d-a585-7f7eb73c5807.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-07 01:00:00</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>29a15853-1fe2-49e6-bcbc-7a28bf4573ed.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name           timestamp  is_major version  \\\n",
       "0    0 2020-01-01 01:00:00      True           \n",
       "1    2 2020-01-03 01:00:00     False           \n",
       "2    4 2020-01-05 01:00:00     False           \n",
       "3    6 2020-01-07 01:00:00     False           \n",
       "\n",
       "                                           file   NONE      C     Q      S  \n",
       "0  e94bf29e-9cc4-42ef-8f06-0315a822d532.parquet  891.0    NaN   NaN    NaN  \n",
       "1  2bd928b5-f354-4958-9410-746774ea2d0e.parquet    NaN  168.0   NaN    NaN  \n",
       "2  20ebdf31-5b8b-4f4d-a585-7f7eb73c5807.parquet    NaN    NaN  77.0    NaN  \n",
       "3  29a15853-1fe2-49e6-bcbc-7a28bf4573ed.parquet    NaN    NaN   NaN  644.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embarkation_by_revision(scores_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0ba1f",
   "metadata": {},
   "source": [
    "Again, as we see, all worked as expected: each scoring job instance have only processed the data from the revision that was unprocessed at the time.\n",
    "\n",
    "Let's change this behavior for demonstration purposes. With the `custom_start_time` argument, I will force the job to obtain all the changes made in the initial dataset since the time 0, not since that last run of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6ec5b72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.704196Z",
     "iopub.status.busy": "2022-12-28T13:20:20.703536Z",
     "iopub.status.idle": "2022-12-28T13:20:20.809346Z",
     "shell.execute_reply": "2022-12-28T13:20:20.809002Z"
    }
   },
   "outputs": [],
   "source": [
    "scores_buffer = MemoryFileSyncer()\n",
    "\n",
    "job = UpdatableDatasetScoringJob(\n",
    "    name = 'scoring',\n",
    "    version = '',\n",
    "    dst_syncer = scores_buffer,\n",
    "    methods = [\n",
    "        UpdatableDatasetScoringMethod(\n",
    "            'passenger_scores',\n",
    "            dataset_buffer,\n",
    "            'passengers',\n",
    "            compute_scores        \n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i in [0,2,4,6]:\n",
    "    job.run(\n",
    "        current_time = time(i)+timedelta(hours=1), \n",
    "        custom_revision_id=str(i),\n",
    "        custom_start_time=time(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec49b03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T13:20:20.812387Z",
     "iopub.status.busy": "2022-12-28T13:20:20.811948Z",
     "iopub.status.idle": "2022-12-28T13:20:20.854766Z",
     "shell.execute_reply": "2022-12-28T13:20:20.855273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_major</th>\n",
       "      <th>version</th>\n",
       "      <th>file</th>\n",
       "      <th>NONE</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>e7badd8a-e8ee-4a7d-9521-f5345fbc2c34.parquet</td>\n",
       "      <td>891.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-03 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>fb9582ff-1e13-4389-b718-696bc88c756b.parquet</td>\n",
       "      <td>723.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-03 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>5587e92b-9d71-4f5e-9502-30737a6f2cdb.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-05 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>325cbab0-1d3e-44ae-86b5-8973677d50b2.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-05 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>bdeb970a-22d3-4036-a0ea-0ae319ef080b.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-05 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>09fb89bd-70cb-4e06-9b89-d7c2ef982857.parquet</td>\n",
       "      <td>646.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-07 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>c29aa577-79c7-4d1c-9442-89ecf3cac2f6.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-07 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>db3d8fa6-1cb9-4de8-a2a5-815b9ae804fd.parquet</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-07 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>4c6683a0-ff24-4b17-aff3-5051d66529f4.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-01-07 01:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>5fc276af-15c0-4344-a4c7-a166329483d8.parquet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name           timestamp  is_major version  \\\n",
       "0    0 2020-01-01 01:00:00      True           \n",
       "1    2 2020-01-03 01:00:00      True           \n",
       "1    2 2020-01-03 01:00:00      True           \n",
       "2    4 2020-01-05 01:00:00      True           \n",
       "2    4 2020-01-05 01:00:00      True           \n",
       "2    4 2020-01-05 01:00:00      True           \n",
       "3    6 2020-01-07 01:00:00      True           \n",
       "3    6 2020-01-07 01:00:00      True           \n",
       "3    6 2020-01-07 01:00:00      True           \n",
       "3    6 2020-01-07 01:00:00      True           \n",
       "\n",
       "                                           file   NONE      C     Q      S  \n",
       "0  e7badd8a-e8ee-4a7d-9521-f5345fbc2c34.parquet  891.0    NaN   NaN    NaN  \n",
       "1  fb9582ff-1e13-4389-b718-696bc88c756b.parquet  723.0    NaN   NaN    NaN  \n",
       "1  5587e92b-9d71-4f5e-9502-30737a6f2cdb.parquet    NaN  168.0   NaN    NaN  \n",
       "2  325cbab0-1d3e-44ae-86b5-8973677d50b2.parquet    NaN    NaN  77.0    NaN  \n",
       "2  bdeb970a-22d3-4036-a0ea-0ae319ef080b.parquet    NaN  168.0   NaN    NaN  \n",
       "2  09fb89bd-70cb-4e06-9b89-d7c2ef982857.parquet  646.0    NaN   NaN    NaN  \n",
       "3  c29aa577-79c7-4d1c-9442-89ecf3cac2f6.parquet    NaN    NaN   NaN  644.0  \n",
       "3  db3d8fa6-1cb9-4de8-a2a5-815b9ae804fd.parquet    2.0    NaN   NaN    NaN  \n",
       "3  4c6683a0-ff24-4b17-aff3-5051d66529f4.parquet    NaN    NaN  77.0    NaN  \n",
       "3  5fc276af-15c0-4344-a4c7-a166329483d8.parquet    NaN  168.0   NaN    NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embarkation_by_revision(scores_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba7ea7",
   "metadata": {},
   "source": [
    "Why are there so many files? Why are the different embarkations always in different files?\n",
    "\n",
    "Because when we read changes from the revisions 0, 2, and 4, there are 3 physical files to read (1 at each revisions). We don't know for sure how these files are organized: maybe they override records of the previous ones, but _maybe_ they just add records, and _maybe_ if we try to read them all at once, we will overload the memory. This is why instead of `read` method,  `read_iter` is used. For the (4) revision, it yields: \n",
    "\n",
    "* first the dataframe from the revision (4), \n",
    "* then the dataframe from the revision (2), minus records in revision (4)\n",
    "* then the dataframe from the revision (0) minus records in revision (2), (0)\n",
    "\n",
    "If the data would be partitioned, it would further increase amount of dataframes.\n",
    "\n",
    "Each of these dataframes has an important property: it is not bigger that something, created by `FeaturizationJob`. Therefore, it will not overload the memory. So regardless of how long this system is run untouched, the data accumulation will not result in memory overflow. This approach should be applied to all other jobs that process `UpdatableDatasets`\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
