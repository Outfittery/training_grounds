{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delivery\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this part, we will deliver the featurization job to a remote server and execute it there. This actually can be done with just few lines of code. But we will show a lot of the process \"under the hood\" to make you familiar with it, and to explain why do we have this setup.\n",
    "\n",
    "Delivery is the most fundamental purpose of training grounds. It is extremely easy to write _some_ data science code, that is executable on your local machine. It is not so easy though to then deliver this code to a remote server (be it server for training or a web-server that exposes model to the world) so that everything continues to work.\n",
    "\n",
    "Delivery in Training Grounds is built upon the following principles.\n",
    "\n",
    "### Deliverables are pickled objects\n",
    "\n",
    "We do not deliver chunks of code or notebooks. Instead, we deliver the objects that incapsulate this code.\n",
    "\n",
    "The most simple way of doing it is write a class that contains all the required functionality in `run` method and deliver it. In the previous presentations you saw that the `FeaturizationJob` class is more complicated. We didn't have the functionality written in the run method; instead, this functionality was defined as a composition of smaller objects, according to SOLID principles. This is *not* a requirement of delivery subsystem, the delivery will work perfectly fine without any SOLID. \n",
    "\n",
    "When prototyping, we would recommend to stick to the simplest way, which is implementing everything in the `run` method. When the solution is developed enough, you may need to consider it's decomposition to the subclasses in order to provide testability and reusability.\n",
    "\n",
    "### The source code is delivered alongside the objects\n",
    "\n",
    "In many frameworks there is a backstage idea that the framework has a comprehensive set of bug-free basic objects, and any imaginable functionality we need can be composed from these. So the users would never need to write Python code ever again, instead they would write declarative descriptions of the functionality they need. In this mindset, the delivery of the source code can be performed with `pip install`.\n",
    "\n",
    "This approach is not the one TG follows due to the various reasons:\n",
    "* Frameworks seldom actually get to this stage of development\n",
    "* Versioning is painful\n",
    "* This mindset creates a complexity gap: to do something new, with no basic objects available, is a lot harder than using the constructor. In this regard, it is extremely important for us that the user can implement this prototyping functionality in the `run` method without using any complex architecture.\n",
    "\n",
    "Therefore, the source code is changing rapidly. Publishing it via PiPy or `git` would create a very complicated setup, when delivery requires a lot of intermediate stages, such as commiting, pushing, tagging or publishing. \n",
    "\n",
    "The simpler solution is to package the current source code into a Python package, placing the pickled objects as resource inside this package. No external actions are required in this case: the object will be unseparable from the source code, thus preventing versioning issues.\n",
    "\n",
    "### Multiple versions\n",
    "\n",
    "We wanted different versions of a model to be able to run at the same time. But how can we do that, if the models are represented as packages? In Python, we cannot have two modules with the same name installed at the same time. Thus, they have to have different name. This is why Training Grounds itself is not a Python package, but a folder inside your project. \n",
    "\n",
    "Consider the file structure, recommended by TG:\n",
    "```\n",
    "/myproject/tg/\n",
    "/myproject/tg/common/\n",
    "/myproject/tg/mylibrary/\n",
    "/myproject/some_other_code_of_the_project\n",
    "```\n",
    "\n",
    "When building a package, these files will be transfomed into something like:\n",
    "```\n",
    "/package_name/UID/\n",
    "/package_name/UID/tg/\n",
    "/package_name/UID/tg/common/\n",
    "/package_name/UID/tg/mylibrary/\n",
    "```\n",
    "\n",
    "Note that everything outside of original `/myproject/tg/` folder is ignored. So outside of `tg` folder you can have data caches, temporal files, sensitive information (as long as it's not pushed in the repository) and so on. It will never be delivered anywhere. The corollary is that all the classes and functions you use in your object must be defined inside `/tg/` folder. Otherwise, they will not be delivered.\n",
    "\n",
    "The name of the TG is actually `UID.tg`, with different UID in each package. Hence, several versions of TG can be used at the same time! But that brings another limitation that must be observed inside `tg` folder: all the references inside TG must be relative. They cannot refer to `tg`, because `tg` will become `{UID}.tg` in the runtime on the remote server.\n",
    "\n",
    "\n",
    "### Hot Module Replacement\n",
    "\n",
    "Now, the question arises, how to use this package. We cannot write something like:\n",
    "\n",
    "```\n",
    "from UID.tg import *\n",
    "```\n",
    "\n",
    "because the name `UID` is formed dynamically. \n",
    "\n",
    "The solution is to install the module during runtime. During this process, the name becomes known, and then we can dynamically import from the module. Of course, importing classes or methods would not be handy, but remember that deliverables are objects, and these objects are pickled as the module resources. So all we need to do is to unpickle these objects, and all the classes and methods will be loaded dynamically by unpickler. \n",
    "\n",
    "This work is done by `EntryPoint` class.\n",
    "\n",
    "#### Note for advanced users\n",
    "\n",
    "When package is created, we pickle the objects under the local version of TG, thus, the classes are unavoidably pickled as `tg.SomeClass`, but we want to unpickle them as `UID.tg.SomeClass`. How is this achived? Fortunately, pickling allows you to do some manipulations while pickling/unpickling, and so we just replace all `tg.` prefixes to `UID.tg.` while building a package (UID is already known at this time).\n",
    "\n",
    "It is also possible to do same trick when unpickling: if you want to transfer the previously packaged object into the current `tg` version, this is possible. Of course, it's on your responsibility to ensure that current TG is compatible with an older version. Later we will discuss a use case for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging\n",
    "\n",
    "Consider the following job we want to deliver to the remote server and execute there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T13:36:01.778577Z",
     "start_time": "2021-04-01T13:36:00.877514Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-08T12:04:39.399484Z",
     "iopub.status.busy": "2021-04-08T12:04:39.398418Z",
     "iopub.status.idle": "2021-04-08T12:04:40.165552Z",
     "shell.execute_reply": "2021-04-08T12:04:40.165908Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tg.common.datasets.featurization import FeaturizationJob, DataframeFeaturizer, InMemoryJobDestination\n",
    "from tg.common.datasets.selectors import Selector\n",
    "from tg.common.datasets.access import MockDfDataSource\n",
    "import pandas as pd\n",
    "\n",
    "destination = InMemoryJobDestination()\n",
    "\n",
    "job = FeaturizationJob(\n",
    "    name = 'job',\n",
    "    version = 'v1',\n",
    "    source = MockDfDataSource(pd.read_csv('titanic.csv')),\n",
    "    featurizers = {\n",
    "        'passengers': DataframeFeaturizer(row_selector = Selector.identity)\n",
    "    },\n",
    "    destination = destination,\n",
    "    status_report_frequency=100\n",
    ")\n",
    "\n",
    "job.run()\n",
    "destination.buffer['passengers'][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details about mentioned classes, we refer you to the previous demos. Essentialy, the code above just passes the `titanik.csv` file through the TG machinery, decomposes and reconstructs it again, without changing anything.\n",
    "\n",
    "Let's build the package with this job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T13:40:05.072611Z",
     "start_time": "2021-04-01T13:40:04.559039Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-08T12:04:40.169677Z",
     "iopub.status.busy": "2021-04-08T12:04:40.169230Z",
     "iopub.status.idle": "2021-04-08T12:04:40.909567Z",
     "shell.execute_reply": "2021-04-08T12:04:40.910361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': <tg.common.delivery.packaging.packaging_dto.PackagingTask at 0x7f5060162050>,\n",
       " 'module_name': 'titanic_featurization__1',\n",
       " 'path': PosixPath('/home/yura/Desktop/repos/boilerplate-service-ml/temp/release/package/titanic_featurization__1-1.tar.gz')}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tg.common.delivery.packaging import PackagingTask, make_package\n",
    "\n",
    "packaging_task = PackagingTask(\n",
    "    name = 'titanic_featurization',\n",
    "    version = '1',\n",
    "    payload = dict(job = job),\n",
    ")\n",
    "\n",
    "info = make_package(packaging_task)\n",
    "info.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `PackagingTask` defines all the properties of the package, and `make_package` creates the package.\n",
    "\n",
    "*NOTE*: `name` and `version` here are the name and version in the sense of Python. \n",
    "\n",
    "If you create and install another package with the name `titanic_featurization` and higher version, the version 1 will be removed from the system - because Python does not allow you to have different versions of the same library at the same time. This is the way to go if you actually want to update the model.\n",
    "\n",
    "If you want several models to be used at the same time, you need to incorporate the version inside name, e.g. `name=titanic_featurization_1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now install the created package. `make_package` stored a file in the local system, and now we will install it. In the code, it results in the `EntryPoint` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T13:43:37.450812Z",
     "start_time": "2021-04-01T13:43:16.250127Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-08T12:04:40.916798Z",
     "iopub.status.busy": "2021-04-08T12:04:40.915498Z",
     "iopub.status.idle": "2021-04-08T12:06:06.407186Z",
     "shell.execute_reply": "2021-04-08T12:06:06.407720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'module_name': 'titanic_featurization',\n",
       " 'module_version': '1',\n",
       " 'tg_module_name': 'titanic_featurization__1.tg',\n",
       " 'python_module_name': 'titanic_featurization__1',\n",
       " 'original_tg_module_name': 'tg',\n",
       " 'resources_location': '/home/yura/anaconda3/envs/bo/lib/python3.7/site-packages/titanic_featurization__1/resources'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tg.common.delivery.packaging import install_package_and_get_loader\n",
    "\n",
    "entry_point = install_package_and_get_loader(info.path)\n",
    "entry_point.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the job from the package. Note that the classes are indeed located in different modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T12:06:06.415939Z",
     "iopub.status.busy": "2021-04-08T12:06:06.415439Z",
     "iopub.status.idle": "2021-04-08T12:06:06.460956Z",
     "shell.execute_reply": "2021-04-08T12:06:06.460335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tg.common.datasets.featurization.featurization_job.FeaturizationJob at 0x7f5035ea1950>,\n",
       " <titanic_featurization__1.tg.common.datasets.featurization.featurization_job.FeaturizationJob at 0x7f5063a42c10>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_job = entry_point.load_resource('job')\n",
    "job, loaded_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Containering\n",
    "\n",
    "Although we could just run the package at the remote server via ssh, the more suitable way is to use Docker. Training Grounds defines methods to build the docker container out of the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T12:06:06.473021Z",
     "iopub.status.busy": "2021-04-08T12:06:06.471760Z",
     "iopub.status.idle": "2021-04-08T12:06:19.177829Z",
     "shell.execute_reply": "2021-04-08T12:06:19.178945Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.common.delivery.packaging import ContaineringTask, make_container\n",
    "\n",
    "ENTRY_FILE_TEMPLATE = '''\n",
    "import {module}.{tg_name}.common.delivery.jobs.ssh_docker_job_execution as feat\n",
    "from {module} import Entry\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(\"Hello, docker!\")\n",
    "job = Entry.load_resource('job')\n",
    "job.run()\n",
    "logger.info(job.destination.buffer['passengers'][0])\n",
    "\n",
    "'''\n",
    "\n",
    "DOCKERFILE_TEMPLATE  = '''FROM python:3.7\n",
    "\n",
    "{install_libraries}\n",
    "\n",
    "COPY . /featurization\n",
    "\n",
    "WORKDIR /featurization\n",
    "\n",
    "COPY {package_filename} package.tar.gz\n",
    "\n",
    "RUN pip install package.tar.gz\n",
    "\n",
    "CMD [\"python3\",\"/featurization/run.py\"]\n",
    "'''\n",
    "\n",
    "task = ContaineringTask(\n",
    "    packaging_task = packaging_task,\n",
    "    entry_file_name = 'run.py',\n",
    "    entry_file_template=ENTRY_FILE_TEMPLATE,\n",
    "    dockerfile_template=DOCKERFILE_TEMPLATE,\n",
    "    image_name='titanic-featurization',\n",
    "    image_tag='test'\n",
    ")\n",
    "\n",
    "make_container(task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run this container locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T12:06:19.191115Z",
     "iopub.status.busy": "2021-04-08T12:06:19.183748Z",
     "iopub.status.idle": "2021-04-08T12:06:23.714779Z",
     "shell.execute_reply": "2021-04-08T12:06:23.715285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:22,651 INFO: Hello, docker!\r\n",
      "2021-04-08 12:06:22,668 INFO: Featurization Job job at version v1 has started\r\n",
      "2021-04-08 12:06:22,668 INFO: Fetching data\r\n",
      "2021-04-08 12:06:22,695 INFO: 100 data objects are processed\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:22,731 INFO: 200 data objects are processed\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:22,767 INFO: 300 data objects are processed\r\n",
      "2021-04-08 12:06:22,798 INFO: 400 data objects are processed\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:22,825 INFO: 500 data objects are processed\r\n",
      "2021-04-08 12:06:22,847 INFO: 600 data objects are processed\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:22,863 INFO: 700 data objects are processed\r\n",
      "2021-04-08 12:06:22,878 INFO: 800 data objects are processed\r\n",
      "2021-04-08 12:06:22,896 INFO: Data fetched, finalizing\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:22,910 INFO: Uploading data\r\n",
      "2021-04-08 12:06:22,910 INFO: Featurization job completed\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:22,911 INFO:      PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\r\n",
      "0              1         0       3  ...   7.2500   NaN         S\r\n",
      "1              2         1       1  ...  71.2833   C85         C\r\n",
      "2              3         1       3  ...   7.9250   NaN         S\r\n",
      "3              4         1       1  ...  53.1000  C123         S\r\n",
      "4              5         0       3  ...   8.0500   NaN         S\r\n",
      "..           ...       ...     ...  ...      ...   ...       ...\r\n",
      "886          887         0       2  ...  13.0000   NaN         S\r\n",
      "887          888         1       1  ...  30.0000   B42         S\r\n",
      "888          889         0       3  ...  23.4500   NaN         S\r\n",
      "889          890         1       1  ...  30.0000  C148         C\r\n",
      "890          891         0       3  ...   7.7500   NaN         Q\r\n",
      "\r\n",
      "[891 rows x 12 columns]\r\n"
     ]
    }
   ],
   "source": [
    "!docker run titanic-featurization:test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `make_container` function is not \"standard\" or \"universal\": it just allows building the containers that are suitable for Sagemaker tasks and featurization jobs. So if you need some more sophisticated containering, please check the source code of this function to understand how to create an analog for it. Most of the complicated job is done by packaging, so `make_container` really just fills templates with values and executes some shell commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSH/Docker routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, you don't really need to do packaging or containering yourself, because we have a higher level level interfaces to do that, which is `Routine` classes. For instance, `SSHDockerJobRoutine` allows you to execute your jobs in the docker at the remote server to which you have ssh access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T12:06:23.722202Z",
     "iopub.status.busy": "2021-04-08T12:06:23.719484Z",
     "iopub.status.idle": "2021-04-08T12:06:23.732169Z",
     "shell.execute_reply": "2021-04-08T12:06:23.731675Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.common.delivery.jobs import SSHDockerJobRoutine, DockerOptions\n",
    "\n",
    "routine = SSHDockerJobRoutine(\n",
    "    job = job,\n",
    "    repository=None,\n",
    "    remote_host_address=None,\n",
    "    remote_host_user=None,\n",
    "    options = DockerOptions(propagate_environmental_variables=[])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the fields are specified to None, because we are not going to actually start the remote job with this notebook. `SSHDockerJobRoutine` allows less intrusive methods of running your code for debugging.\n",
    "\n",
    "Using the `.attached` accesor, we can run job in the same Python process that your current code is executed. This is, of course, the fastest way to do that, and therefore it's preferrable to use this to debug for typos, wrong logic, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T12:06:23.738319Z",
     "iopub.status.busy": "2021-04-08T12:06:23.737788Z",
     "iopub.status.idle": "2021-04-08T12:06:23.904931Z",
     "shell.execute_reply": "2021-04-08T12:06:23.905309Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,736 INFO: Featurization Job job at version v1 has started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,740 INFO: Fetching data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,766 INFO: 100 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,785 INFO: 200 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,797 INFO: 300 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,813 INFO: 400 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,828 INFO: 500 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,849 INFO: 600 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,859 INFO: 700 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,872 INFO: 800 data objects are processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,885 INFO: Data fetched, finalizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,899 INFO: Uploading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-08 14:06:23,901 INFO: Featurization job completed\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO)\n",
    "\n",
    "routine.attached.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.local` accessor builds package and container, then executes the container locally. This step allows debugging the following things:\n",
    "\n",
    "* If your job is serializable. This is usually achievable by not using `lambda` syntax.\n",
    "* If all the code the job uses is located inside the TG folder, and if all the references are relative. If something is wrong, you will see the import error.\n",
    "* If the environmental variables are carried to docker correctly. \n",
    "* If you have sufficient permissions to start docker\n",
    "* etc.\n",
    "\n",
    "This step allows you to check the deliverability of your work. \n",
    "\n",
    "Unfortunately, Jupyther notebook does not allow to view the output of `subprocess.call`, so the next cell will not produce an output. When running from command line, you'll be able to see the output of packaging, containering and then the output of the running container.\n",
    "\n",
    "The execution may take a while since there are many packages TG requires. You can check the progress in the console from which `jupyter notebook` was started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T12:06:23.910289Z",
     "iopub.status.busy": "2021-04-08T12:06:23.909845Z",
     "iopub.status.idle": "2021-04-08T12:06:38.513021Z",
     "shell.execute_reply": "2021-04-08T12:06:38.513581Z"
    }
   },
   "outputs": [],
   "source": [
    "routine.local.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you can retrieve logs from the container with the following useful method. Note that logs printed via `logging` are placed in stderr instead of strdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-08T12:06:38.524436Z",
     "iopub.status.busy": "2021-04-08T12:06:38.522572Z",
     "iopub.status.idle": "2021-04-08T12:06:38.656285Z",
     "shell.execute_reply": "2021-04-08T12:06:38.656764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 12:06:37,555 INFO: Welcome to Training Grounds. This is Job execution via Docker/SSH\n",
      "2021-04-08 12:06:37,568 INFO: Executing job job version v1\n",
      "2021-04-08 12:06:37,568 INFO: Featurization Job job at version v1 has started\n",
      "2021-04-08 12:06:37,569 INFO: Fetching data\n",
      "2021-04-08 12:06:37,591 INFO: 100 data objects are processed\n",
      "2021-04-08 12:06:37,623 INFO: 200 data objects are processed\n",
      "2021-04-08 12:06:37,646 INFO: 300 data objects are processed\n",
      "2021-04-08 12:06:37,673 INFO: 400 data objects are processed\n",
      "2021-04-08 12:06:37,692 INFO: 500 data objects are processed\n",
      "2021-04-08 12:06:37,715 INFO: 600 data objects are processed\n",
      "2021-04-08 12:06:37,757 INFO: 700 data objects are processed\n",
      "2021-04-08 12:06:37,799 INFO: 800 data objects are processed\n",
      "2021-04-08 12:06:37,831 INFO: Data fetched, finalizing\n",
      "2021-04-08 12:06:37,842 INFO: Uploading data\n",
      "2021-04-08 12:06:37,842 INFO: Featurization job completed\n",
      "2021-04-08 12:06:37,842 INFO: Job completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output, errors = routine.local.get_logs()\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`routine.remote` has the same interface as `routine.local`, and will run the container at the remote machine. The only problems you should have at these stage are permissions:\n",
    "* to push to your docker registry\n",
    "* to connect to the remote machine via SSH\n",
    "* to execute `docker run` at the remote machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this demo, we delivered the job to the remote server and executed it there. That concludes the featurization-related part of the Training Grounds.\n",
    "\n",
    "Note that the packaging and containering techniques are not specific for the featurization, and can process any code. In the subsequent demos, the same techniques will be applied to run the training on the remote server as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
